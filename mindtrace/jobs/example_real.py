"""
Example demonstrating the new Consumer API and JobSchema-based routing system
using ALL real backend implementations: Local, Redis, and RabbitMQ.
"""

import sys
import os
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from pydantic import BaseModel
from typing import List, Dict, Any
from mindtrace.consumer import Consumer
from mindtrace.queue_management.orchestrator import Orchestrator
from mindtrace.queue_management.local.client import LocalClient
from mindtrace.queue_management.redis.client import RedisClient
from mindtrace.queue_management.rabbitmq.client import RabbitMQClient
from mindtrace.types import JobSchema, JobInput, JobOutput, Job
from mindtrace.utils import job_from_schema


# Define specific input/output models for different job types
class ObjectDetectionInput(JobInput):
    """Input for object detection jobs."""
    image_path: str
    confidence_threshold: float = 0.5
    model_name: str = "yolov8"
    customer_id: str
    production_line: str


class ObjectDetectionOutput(JobOutput):
    """Output for object detection jobs."""
    detections: List[Dict[str, Any]]
    processing_time: float
    model_version: str


class DataProcessingInput(JobInput):
    """Input for data processing jobs."""
    dataset_path: str
    processing_type: str
    batch_size: int = 100
    filters: Dict[str, Any] = {}


class DataProcessingOutput(JobOutput):
    """Output for data processing jobs."""
    processed_records: int
    output_path: str
    errors: List[str] = []


# Define specialized Consumer classes
class CustomerALine4Consumer(Consumer):
    """Consumer for Customer A Line 4 object detection jobs."""
    
    def __init__(self):
        super().__init__("customer_a_line_4_object_detection")
    
    def run(self, job: Job) -> None:
        """Process Customer A Line 4 object detection job."""
        print(f"ğŸ” Processing Customer A Line 4 object detection job: {job.id}")
        
        # Extract input data
        input_data = job.payload.input
        if isinstance(input_data, ObjectDetectionInput):
            print(f"   ğŸ“· Image: {input_data.image_path}")
            print(f"   ğŸ¯ Confidence: {input_data.confidence_threshold}")
            print(f"   ğŸ­ Customer: {input_data.customer_id}, Line: {input_data.production_line}")
            
            # Simulate processing
            print("   âš™ï¸ Running object detection...")
            print("   âœ… Object detection completed!")


class CustomerBDataProcessingConsumer(Consumer):
    """Consumer for Customer B data processing jobs."""
    
    def __init__(self):
        super().__init__("customer_b_data_processing")
    
    def run(self, job: Job) -> None:
        """Process Customer B data processing job."""
        print(f"ğŸ“Š Processing Customer B data processing job: {job.id}")
        
        # Extract input data
        input_data = job.payload.input
        if isinstance(input_data, DataProcessingInput):
            print(f"   ğŸ“ Dataset: {input_data.dataset_path}")
            print(f"   ğŸ”§ Processing type: {input_data.processing_type}")
            print(f"   ğŸ“¦ Batch size: {input_data.batch_size}")
            
            # Simulate processing
            print("   âš™ï¸ Processing data...")
            print("   âœ… Data processing completed!")


def test_backend(backend_name: str, backend_client, test_redis_rabbitmq: bool = False):
    """Test a specific backend implementation."""
    print(f"\n{'='*50}")
    print(f"ğŸš€ Testing {backend_name} Backend")
    print(f"{'='*50}")
    
    try:
        # 1. Create orchestrator with the specific backend
        print(f"1ï¸âƒ£ Setting up orchestrator with {backend_name} backend...")
        orchestrator = Orchestrator(backend_client)
        print(f"   âœ… Real orchestrator created with {backend_name} backend\n")
        
        # 2. Define JobSchemas for different types of work
        print("2ï¸âƒ£ Defining JobSchemas...")
        
        # Schema for Customer A Line 4 object detection
        customer_a_line_4_schema = JobSchema(
            name="customer_a_line_4_object_detection",
            input=ObjectDetectionInput(
                image_path="",
                customer_id="customer_a", 
                production_line="line_4"
            ),
            output=ObjectDetectionOutput(
                detections=[],
                processing_time=0.0,
                model_version=""
            )
        )
        
        # Schema for Customer B data processing
        customer_b_data_schema = JobSchema(
            name="customer_b_data_processing",
            input=DataProcessingInput(
                dataset_path="",
                processing_type=""
            ),
            output=DataProcessingOutput(
                processed_records=0,
                output_path=""
            )
        )
        
        print(f"   ğŸ“‹ Schema 1: {customer_a_line_4_schema.name}")
        print(f"   ğŸ“‹ Schema 2: {customer_b_data_schema.name}")
        print("   âœ… JobSchemas defined\n")
        
        # 3. Register schemas with orchestrator
        print(f"3ï¸âƒ£ Registering schemas with {backend_name} orchestrator...")
        queue_1 = orchestrator.register(customer_a_line_4_schema)
        queue_2 = orchestrator.register(customer_b_data_schema)
        print(f"   ğŸ—‚ï¸ {backend_name} queue created for schema 1: {queue_1}")
        print(f"   ğŸ—‚ï¸ {backend_name} queue created for schema 2: {queue_2}")
        print(f"   âœ… Schemas registered with {backend_name} backend\n")
        
        # 4. Create and connect consumers
        print(f"4ï¸âƒ£ Creating and connecting consumers to {backend_name}...")
        
        consumer_a = CustomerALine4Consumer()
        consumer_b = CustomerBDataProcessingConsumer()
        
        consumer_a.connect(orchestrator)
        consumer_b.connect(orchestrator)
        
        print(f"   ğŸ¤– Consumer A connected to {backend_name} queue: {consumer_a.queue_name}")
        print(f"   ğŸ¤– Consumer B connected to {backend_name} queue: {consumer_b.queue_name}")
        print(f"   âœ… Consumers connected to {backend_name} orchestrator\n")
        
        # 5. Create jobs using job_from_schema utility
        print("5ï¸âƒ£ Creating jobs using job_from_schema utility...")
        
        # Create Customer A Line 4 job
        detection_input = ObjectDetectionInput(
            image_path=f"/data/customer_a/line_4/image_{backend_name.lower()}.jpg",
            confidence_threshold=0.7,
            model_name="yolov8n",
            customer_id="customer_a",
            production_line="line_4"
        )
        detection_job = job_from_schema(customer_a_line_4_schema, detection_input)
        
        # Create Customer B data processing job
        processing_input = DataProcessingInput(
            dataset_path=f"/data/customer_b/batch_{backend_name.lower()}.csv",
            processing_type="feature_extraction",
            batch_size=50,
            filters={"status": "active", "type": "sensor_data", "backend": backend_name.lower()}
        )
        processing_job = job_from_schema(customer_b_data_schema, processing_input)
        
        print(f"   ğŸ“„ Created detection job for {backend_name}: {detection_job.id}")
        print(f"   ğŸ“„ Created processing job for {backend_name}: {processing_job.id}")
        print(f"   âœ… Jobs created for {backend_name} using real job_from_schema utility\n")
        
        # 6. Publish jobs to appropriate queues using real backend
        print(f"6ï¸âƒ£ Publishing jobs to {backend_name} queues...")
        
        job_id_1 = orchestrator.publish(queue_1, detection_job)
        job_id_2 = orchestrator.publish(queue_2, processing_job)
        
        print(f"   ğŸ“¤ Published detection job to {backend_name} queue: {queue_1} (ID: {job_id_1})")
        print(f"   ğŸ“¤ Published processing job to {backend_name} queue: {queue_2} (ID: {job_id_2})")
        print(f"   âœ… Jobs published to {backend_name} backend\n")
        
        # 7. Check queue status
        print(f"7ï¸âƒ£ Checking {backend_name} queue status...")
        queue_1_count = orchestrator.count_queue_messages(queue_1)
        queue_2_count = orchestrator.count_queue_messages(queue_2)
        print(f"   ğŸ“Š {backend_name} queue {queue_1} has {queue_1_count} messages")
        print(f"   ğŸ“Š {backend_name} queue {queue_2} has {queue_2_count} messages")
        print(f"   âœ… {backend_name} queue status checked\n")
        
        # 8. Process jobs with consumers using real backend
        print(f"8ï¸âƒ£ Processing jobs with {backend_name} consumers...")
        print(f"\n--- Consumer A Processing ({backend_name} Backend) ---")
        consumer_a.consume(num_messages=1)
        
        print(f"\n--- Consumer B Processing ({backend_name} Backend) ---")
        consumer_b.consume(num_messages=1)
        
        print(f"\n   âœ… Jobs processed using {backend_name} implementation\n")
        
        # 9. Check queue status after processing
        print(f"9ï¸âƒ£ Checking {backend_name} queue status after processing...")
        queue_1_count_after = orchestrator.count_queue_messages(queue_1)
        queue_2_count_after = orchestrator.count_queue_messages(queue_2)
        print(f"   ğŸ“Š {backend_name} queue {queue_1} now has {queue_1_count_after} messages")
        print(f"   ğŸ“Š {backend_name} queue {queue_2} now has {queue_2_count_after} messages")
        print(f"   âœ… {backend_name} queues processed successfully\n")
        
        print(f"ğŸ‰ {backend_name} backend test completed successfully!")
        
        return True
        
    except Exception as e:
        print(f"âŒ Error testing {backend_name} backend: {str(e)}")
        if test_redis_rabbitmq:
            print(f"   â„¹ï¸ This is expected if {backend_name} is not running locally")
        return False


def main():
    """Demonstrate the new Consumer API with all real backend implementations."""
    print("ğŸš€ Demonstrating new Consumer API with ALL REAL backends\n")
    print("   ğŸ  Local Backend (always available)")
    print("   ğŸ”´ Redis Backend (requires Redis server)")
    print("   ğŸ° RabbitMQ Backend (requires RabbitMQ server)")
    print("\n" + "="*60)
    
    results = {}
    
    # Test 1: Local Backend (always works)
    print("\nğŸ“¦ LOCAL BACKEND TEST")
    local_backend = LocalClient()
    results['Local'] = test_backend("Local", local_backend, False)
    
    # Test 2: Redis Backend (may fail if Redis not running)
    print("\nğŸ“¦ REDIS BACKEND TEST")
    try:
        redis_backend = RedisClient(host="localhost", port=6379, db=0)
        results['Redis'] = test_backend("Redis", redis_backend, True)
    except Exception as e:
        print(f"âŒ Redis backend connection failed: {str(e)}")
        print("   â„¹ï¸ Make sure Redis server is running: `redis-server`")
        results['Redis'] = False
    
    # Test 3: RabbitMQ Backend (may fail if RabbitMQ not running)
    print("\nğŸ“¦ RABBITMQ BACKEND TEST")
    try:
        rabbitmq_backend = RabbitMQClient(
            host="localhost", 
            port=5672, 
            username="user", 
            password="password"
        )
        results['RabbitMQ'] = test_backend("RabbitMQ", rabbitmq_backend, True)
    except Exception as e:
        print(f"âŒ RabbitMQ backend connection failed: {str(e)}")
        print("   â„¹ï¸ Make sure RabbitMQ server is running: `sudo systemctl start rabbitmq-server`")
        results['RabbitMQ'] = False
    
    # Summary
    print("\n" + "="*60)
    print("ğŸ“Š BACKEND TEST RESULTS SUMMARY")
    print("="*60)
    
    for backend, success in results.items():
        status = "âœ… PASSED" if success else "âŒ FAILED"
        print(f"   {backend:10} Backend: {status}")
    
    successful_backends = [name for name, success in results.items() if success]
    
    print(f"\nğŸ¯ Summary: {len(successful_backends)}/{len(results)} backends tested successfully")
    
    if successful_backends:
        print(f"âœ… Working backends: {', '.join(successful_backends)}")
    
    print("\nğŸ”Ÿ Key advantages demonstrated across ALL backends:")
    print("   ğŸ¯ Specific queues per customer/line combination")
    print("   ğŸ”§ No hardcoded JobType enum - fully extensible")
    print("   ğŸ¤– Simple Consumer.run() interface for job processing")
    print("   ğŸ“‹ JobSchema-based routing with real backends ensures type safety")
    print("   ğŸ—ï¸ Easy to add new job types without code changes")
    print("   ğŸ”„ Automatic queue creation when schemas are registered")
    print("   ğŸ”€ SAME API works across Local, Redis, and RabbitMQ!")
    print("   âš¡ Backend-agnostic Consumer implementation")
    print("   ğŸ’¾ Real message persistence across different infrastructures")
    
    print("\n1ï¸âƒ£1ï¸âƒ£ Clean queue names (no 'schema_' prefix) work on ALL backends:")
    print("   ğŸ“‚ Queue: 'customer_a_line_4_object_detection'")
    print("   ğŸ“‚ Queue: 'customer_b_data_processing'")
    
    print(f"\nğŸ‰ Demo completed! The same Consumer API works seamlessly across all {len(results)} backend types!")


if __name__ == "__main__":
    main() 